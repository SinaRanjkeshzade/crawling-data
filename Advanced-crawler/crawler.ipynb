{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85639c7",
   "metadata": {},
   "source": [
    "# Advanced Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620bcb2",
   "metadata": {},
   "source": [
    "## Crawling and scraping a webpage in amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a0d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find product title. The content may be dynamically loaded.\n",
      "Failed to find price. The content may be dynamically loaded.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Amazon product page\n",
    "url = \"https://www.amazon.com/Atomic-Habits-Proven-Build-Break/dp/0735211299/?_encoding=UTF8&pd_rd_w=fifPC&content-id=amzn1.sym.f2128ffe-3407-4a64-95b5-696504f68ca1&pf_rd_p=f2128ffe-3407-4a64-95b5-696504f68ca1&pf_rd_r=93MK9W7SB3AATE6Y30BX&pd_rd_wg=iexfr&pd_rd_r=1866f822-5759-48aa-aed7-33eb83d8a0b6&ref_=pd_hp_d_btf_crs_zg_bs_283155\"\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Attempt to extract product title and price\n",
    "    title = soup.find('span', id='productTitle')\n",
    "    price = soup.find('span', class_='a-offscreen')  # Common class for price\n",
    "\n",
    "    # Check if elements were found\n",
    "    if title:\n",
    "        print(\"Product Title:\", title.get_text(strip=True))\n",
    "    else:\n",
    "        print(\"Failed to find product title. The content may be dynamically loaded.\")\n",
    "\n",
    "    if price:\n",
    "        print(\"Price:\", price.get_text(strip=True))\n",
    "    else:\n",
    "        print(\"Failed to find price. The content may be dynamically loaded.\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error fetching the webpage: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae6170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Title: Atomic Habits: An Easy & Proven Way to Build Good Habits & Break Bad Ones\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.amazon.com/Atomic-Habits-Proven-Build-Break/dp/0735211299/?_encoding=UTF8&pd_rd_w=fifPC&content-id=amzn1.sym.f2128ffe-3407-4a64-95b5-696504f68ca1&pf_rd_p=f2128ffe-3407-4a64-95b5-696504f68ca1&pf_rd_r=93MK9W7SB3AATE6Y30BX&pd_rd_wg=iexfr&pd_rd_r=1866f822-5759-48aa-aed7-33eb83d8a0b6&ref_=pd_hp_d_btf_crs_zg_bs_283155\" \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    # Adding the \"Accept-Language\" header helps ensure Amazon returns the English version of the page,\n",
    "    # which can prevent issues where content is dynamically loaded or hidden based on region or language.\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Example: Extract the book title\n",
    "    title_tag = soup.find(\"span\", id=\"productTitle\")\n",
    "    if title_tag:\n",
    "        print(\"Book Title:\", title_tag.get_text(strip=True))\n",
    "    else:\n",
    "        print(\"Book title not found.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476de747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Title: Atomic Habits: An Easy & Proven Way to Build Good Habits & Break Bad Ones\n",
      "price text:  \n",
      "price text:  11\n",
      "price text:  \n",
      "price text:  \n",
      "Failed to find price. The price element may not be present or is dynamically loaded with a different structure.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "USER_AGENTS = [\n",
    "    # A few common user agents\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "user_agent = random.choice(USER_AGENTS)\n",
    "\n",
    "# try:\n",
    "import undetected_chromedriver as uc\n",
    "# Set up Selenium with Chrome in headless mode\n",
    "chrome_options = Options()\n",
    "chrome_binary = \"/opt/google/chrome/chrome\"\n",
    "chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "chrome_options.add_argument('--headless')         # Run Chrome without GUI\n",
    "chrome_options.add_argument('--disable-gpu')      # Disable GPU acceleration\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = uc.Chrome(options=chrome_options, browser_executable_path=chrome_binary)\n",
    "\n",
    "# Navigate to the webpage\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the product title to load (max 10 seconds)\n",
    "title_element = WebDriverWait(driver, 50).until(\n",
    "    EC.presence_of_element_located((By.ID, 'productTitle'))\n",
    ")\n",
    "print(\"Product Title:\", title_element.text.strip())\n",
    "\n",
    "# Try multiple strategies to find the price\n",
    "price = None\n",
    "price_selectors = [\n",
    "    (By.CLASS_NAME, 'a-offscreen'),  # Common price class\n",
    "    (By.ID, 'price'),  # ID for main price\n",
    "    (By.CLASS_NAME, 'a-price-whole'),  # Whole price part\n",
    "    (By.XPATH, \"//span[contains(@class, 'a-price') and contains(@class, 'a-text-price')]//span\"),  # Price with discount\n",
    "    (By.XPATH, \"//span[@data-a-size='xl']//span[@class='a-offscreen']\")  # Larger price display\n",
    "]\n",
    "\n",
    "for by, selector in price_selectors:\n",
    "    try:\n",
    "        price_element = WebDriverWait(driver, 50).until(\n",
    "            EC.presence_of_element_located((by, selector))\n",
    "        )\n",
    "        price_text = price_element.text.strip()\n",
    "        print(\"price text: \", price_text)\n",
    "        if price_text and '$' in price_text:  # Ensure it's a valid price\n",
    "            price = price_text\n",
    "            break\n",
    "    except TimeoutException:\n",
    "        continue\n",
    "\n",
    "if price:\n",
    "    print(\"Price:\", price)\n",
    "else:\n",
    "    print(\"Failed to find price. The price element may not be present or is dynamically loaded with a different structure.\")\n",
    "# except TimeoutException:\n",
    "#     print(\"Failed to load elements. The page may not have loaded correctly or elements are missing.\")\n",
    "# except WebDriverException as e:\n",
    "#     print(f\"Error with WebDriver: {e}\")\n",
    "# finally:\n",
    "#     # Clean up by closing the browser\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a2d81",
   "metadata": {},
   "source": [
    "## Crawling Goodreads.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16fb6c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searched for 'data science' successfully.\n",
      "Found 20 search results.\n",
      "1. Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking by Foster Provost\n",
      "2. Data Smart: Using Data Science to Transform Information into Insight by John W. Foreman\n",
      "3. Data Science from Scratch: First Principles with Python by Joel Grus\n",
      "4. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data by Hadley Wickham\n",
      "5. Mindmasters: The Data-Driven Science of Predicting and Changing Human Behavior by Sandra Matz\n",
      "6. Doing Data Science: Straight Talk from the Frontline by Cathy O'Neil\n",
      "7. Machine Learning For Absolute Beginners: A Plain English Introduction (Second Edition) by Oliver Theobald\n",
      "8. Python Data Science Handbook: Essential Tools for Working with Data by Jake VanderPlas\n",
      "9. Numsense! Data Science for the Layman: No Math Added by Annalyn Ng\n",
      "10. Data Science (The MIT Press Essential Knowledge series) by John D. Kelleher\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Prepare headers and cookies for the request ---\n",
    "headers = {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9,fa-IR;q=0.8,fa;q=0.7\",\n",
    "    \"referer\": \"https://www.goodreads.com/book/show/40121378-atomic-habits?ac=1&from_search=true&qid=okcu46oZQB&rank=1\",\n",
    "    \"sec-ch-ua\": '\"Not)A;Brand\";v=\"8\", \"Chromium\";v=\"138\", \"Google Chrome\";v=\"138\"',\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": '\"Windows\"',\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "# You may need to update the cookies below with valid session cookies from your browser if authentication is required\n",
    "cookies = {\n",
    "    \"ccsid\": \"492-7803361-5127910\",\n",
    "    \"locale\": \"en\",\n",
    "    \"blocking_sign_in_interstitial\": \"true\",\n",
    "    \"csm-sid\": \"140-1904891-1149508\",\n",
    "    \"__qca\": \"P1-dcf4d398-b23a-409a-a59d-eb03958544de\",\n",
    "    \"session-id\": \"139-4638899-0436333\",\n",
    "    \"csm-hit\": \"tb:s-VK2SR48DSB4YR958HVRT|1753281489948&t:1753281489949\",\n",
    "    \"ubid-main\": \"132-4139823-5053422\",\n",
    "    \"lc-main\": \"en_US\",\n",
    "    \"session-id-time\": \"2384001530l\",\n",
    "    \"session-token\": 'u65B3YcOHKls1AxXOCfKGuVaAp1yKDJmitfEZmMHLyx4WfkOOsH5FDmuQJGh9dA/rFSSecAzTrkJexsobtadH/oamEcY1vvltjEUcopjyzcnfSSv30ennQltlT9WkUu63FMjIrEaEQhTr34BjZoGh4v/+tG558lF2T8jRfWyHA7JY98O+DiT2ddvAjKpN8TbffR9Yejpoxydi4Mm7ymcdz94nIdsT57eGlVtgSDsS47J3CuGOCWtUSuwtjyVK64Y+ZLxWc2MxmaFBcYu/msSajgYPwf4eNnvkktaTvn/17fKYzY521DksPCGHePJ8/eI7JLzacxYRB+o2ox1J+60HwvdDAyyf0e5DTEZABJtucfWKBiO/5R3yQ==',\n",
    "    \"x-main\": \"cuan1qUXnegr6@riNsHN7b47Ck1?cj9ETfvl1t1ssagMDkjY??UiAH9Kb1FgIQ?c\",\n",
    "    \"at-main\": \"Atza|IwEBIO92cFCkZUBPNU77aExHRArakpB05s0gtzdONR0LC_9ImsTegmhue2_fAV8sP0UtKGGBvVDY810io19Ao7NRAah0k6nm1VbHPIZ0LvDrxW0YEz-kZ5DpiQ7L38pGGxITohzP4jRqIay4icj9mQ63D-8aXpT0MlaxN1AvFYHoD5dMzuSsOV8imxgzM0-kx7TrACJFl6xgVQUVQJZT35gvkuyg0ZXhHl8T60DdjC1wx60qySdA8PWDEQNhysdIM-6xWhU\",\n",
    "    \"sess-at-main\": \"2O1lPqp43Pd0oeUvVdq0SGLxnE1K5mNvYdv/gtlNeI4=\",\n",
    "    \"likely_has_account\": \"true\",\n",
    "    \"_session_id2\": \"4cb74df2b59f2989f81fe9fdbbc1fcae\",\n",
    "    \"jwt_token\": \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6ImZSNXpfWTVjYXZQMllsaXU3eks0YUNJVEJPcVBWdGtxTE9XVURfV3dGOTQifQ.eyJpc3MiOiJodHRwczovL3d3dy5nb29kcmVhZHMuY29tIiwic3ViIjoia2NhOi8vcHJvZmlsZTpnb29kcmVhZHMvQTE5MzBBT05CQ09RTlUiLCJhdWQiOiI2M2RjMjRlN2M2MTFlNmYxNzkyZjgxMzA1OGYyMTU2MGJkOGM2OTM4ZDU0YS5nb29kcmVhZHMuY29tIiwidXNlcl9pZCI6MTkyMzI4MjkyLCJyb2xlIjoidXNlciIsIm5vbmNlIjpudWxsLCJleHAiOjE3NTMyODU1OTIsImlhdCI6MTc1MzI4NTI5Mn0.jLk1Q6Did_zPUkQNC5khGgQWxlKPE8V_aWVECZ3SAAGf28zK8eVXhxfBg6hdlG7_nGvcJpnunpOQRwaaphtnqk2CJdSzvU5H3jLEMZyzjNuFHa2XX4FerdpSdJrJs14mcvK0cDfe6woBwNbYyHzKnci5SjPxeB-DqhQ4nDF7i59fQwBXk7DMz5GkLA5axJb2xWMOOhgLToR_smMQbSVkD4OPVGEIGTdup47XfNZhm30q3bCoePpQ90LEbW9taFESHj1oXMTJzHSGPqCp7btLJUS5_s7-37cy1CgyRbx6Aftnd1Md37QqbsHV5w1NfJfAGC11z7m5w9mlhzNqc8_8eA\"\n",
    "}\n",
    "\n",
    "# --- Perform the search ---\n",
    "keyword = \"data science\"\n",
    "search_url = f\"https://www.goodreads.com/search?q={requests.utils.quote(keyword)}&ref=nav_sb_noss_l_4\"\n",
    "\n",
    "books = []  # List to store all book details\n",
    "\n",
    "response = requests.get(search_url, headers=headers, cookies=cookies)\n",
    "if response.status_code == 200:\n",
    "    print(f\"Searched for '{keyword}' successfully.\")\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Try to find the results table\n",
    "    results_table = soup.select_one(\"table.tableList\")\n",
    "    if results_table:\n",
    "        rows = results_table.select(\"tr\")\n",
    "        print(f\"Found {len(rows)} search results.\")\n",
    "        for idx, row in enumerate(rows[:10], 1):  # Show up to 10 results\n",
    "            # Extract book title and URL\n",
    "            title_elem = row.select_one(\"a.bookTitle\")\n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"N/A\"\n",
    "            book_url = (\n",
    "                \"https://www.goodreads.com\" + title_elem[\"href\"]\n",
    "                if title_elem and title_elem.has_attr(\"href\")\n",
    "                else \"N/A\"\n",
    "            )\n",
    "\n",
    "            # Extract author name and URL\n",
    "            author_elem = row.select_one(\"a.authorName\")\n",
    "            author = author_elem.get_text(strip=True) if author_elem else \"N/A\"\n",
    "            author_url = (\n",
    "                author_elem[\"href\"] if author_elem and author_elem.has_attr(\"href\") else \"N/A\"\n",
    "            )\n",
    "\n",
    "            # Extract image source (cover)\n",
    "            image_elem = row.select_one(\"img.bookCover\")\n",
    "            image_src = image_elem[\"src\"] if image_elem and image_elem.has_attr(\"src\") else \"N/A\"\n",
    "\n",
    "            # Extract minirating and other details\n",
    "            minirating_elem = row.select_one(\"span.minirating\")\n",
    "            minirating = minirating_elem.get_text(strip=True) if minirating_elem else \"N/A\"\n",
    "\n",
    "            # Extract publication year if available\n",
    "            pub_year = \"N/A\"\n",
    "            greytext_elem = row.select_one(\"span.greyText.smallText.uitext\")\n",
    "            if greytext_elem:\n",
    "                import re\n",
    "                match = re.search(r'published\\s+(\\d{4})', greytext_elem.get_text())\n",
    "                if match:\n",
    "                    pub_year = match.group(1)\n",
    "\n",
    "            # Extract number of editions if available\n",
    "            editions_elem = row.select_one(\"a.greyText[href*='/work/editions/']\")\n",
    "            editions = editions_elem.get_text(strip=True) if editions_elem else \"N/A\"\n",
    "\n",
    "            # Append book details to the list\n",
    "            books.append({\n",
    "                \"title\": title,\n",
    "                \"book_url\": book_url,\n",
    "                \"author\": author,\n",
    "                \"author_url\": author_url,\n",
    "                \"image_src\": image_src,\n",
    "                \"minirating\": minirating,\n",
    "                \"pub_year\": pub_year,\n",
    "                \"editions\": editions\n",
    "            })\n",
    "\n",
    "            print(f\"{idx}. {title} by {author}\")\n",
    "        # Optionally, print the books list or use it elsewhere\n",
    "        # print(books)\n",
    "    else:\n",
    "        print(\"No search results table found on the page.\")\n",
    "else:\n",
    "    print(f\"Failed to search. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e987db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.goodreads.com/book/show/17912916-data-science-for-business?from_search=true&from_srp=true&qid=EVSW0x6JUR&rank=1\"\n",
    "resp = requests.get(url)\n",
    "with open(\"page_content.html\", \"w\") as f:\n",
    "    f.write(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7dc45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'authors': ['Foster Provost', 'Tom Fawcett', 'Foster Provost'],\n",
      " 'book_statistics': {},\n",
      " 'comments': [],\n",
      " 'description': 'Read 177 reviews from the world’s largest community \\n'\n",
      "                '    for readers. Written by renowned data science experts '\n",
      "                'Foster Provost and Tom Fawcett, Data Science for …',\n",
      " 'edition_details': None,\n",
      " 'genres': ['Business',\n",
      "            'Nonfiction',\n",
      "            'Technology',\n",
      "            'Science',\n",
      "            'Computer Science',\n",
      "            'Programming',\n",
      "            'Technical',\n",
      "            'GenresBusinessNonfictionTechnologyScienceComputer '\n",
      "            'ScienceProgrammingTechnical',\n",
      "            'Genres',\n",
      "            '...more'],\n",
      " 'isbn': '9781449361327',\n",
      " 'more_information': {},\n",
      " 'number_of_pages': 413,\n",
      " 'original_title': None,\n",
      " 'other_editions_links': [],\n",
      " 'people_reading': [],\n",
      " 'published': None,\n",
      " 'rating': 4.13,\n",
      " 'rating_count': 2586,\n",
      " 'suggested_books': [],\n",
      " 'title': 'Data Science for Business: What You Need to Know about Data Mining '\n",
      "          'and Data-Analytic Thinking'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"page_content.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "extracted = {}\n",
    "\n",
    "# --- Title ---\n",
    "# 1. h1 tag\n",
    "title = None\n",
    "h1 = soup.find(\"h1\", {\"data-testid\": \"bookTitle\"})\n",
    "if h1:\n",
    "    title = h1.get_text(strip=True)\n",
    "# 2. meta og:title\n",
    "if not title:\n",
    "    meta_og_title = soup.find(\"meta\", property=\"og:title\")\n",
    "    if meta_og_title and meta_og_title.get(\"content\"):\n",
    "        title = meta_og_title[\"content\"]\n",
    "# 3. meta twitter:title\n",
    "if not title:\n",
    "    meta_tw_title = soup.find(\"meta\", attrs={\"name\": \"twitter:title\"})\n",
    "    if meta_tw_title and meta_tw_title.get(\"content\"):\n",
    "        title = meta_tw_title[\"content\"]\n",
    "# 4. Schema.org script\n",
    "schema_title = None\n",
    "schema_data = None\n",
    "for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "    try:\n",
    "        data = json.loads(script.string)\n",
    "        if isinstance(data, dict) and data.get(\"@type\") == \"Book\":\n",
    "            schema_data = data\n",
    "            schema_title = data.get(\"name\")\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "if not title and schema_title:\n",
    "    title = schema_title\n",
    "extracted[\"title\"] = title\n",
    "\n",
    "# --- Authors ---\n",
    "authors = []\n",
    "# 1. ContributorLink__name spans\n",
    "for a in soup.select(\"a.ContributorLink span.ContributorLink__name[data-testid='name']\"):\n",
    "    name = a.get_text(strip=True)\n",
    "    if name:\n",
    "        authors.append(name)\n",
    "# 2. Schema.org\n",
    "if schema_data and \"author\" in schema_data:\n",
    "    schema_authors = schema_data[\"author\"]\n",
    "    if isinstance(schema_authors, dict):\n",
    "        schema_authors = [schema_authors]\n",
    "    for author in schema_authors:\n",
    "        if isinstance(author, dict) and author.get(\"@type\") == \"Person\":\n",
    "            name = author.get(\"name\")\n",
    "            if name and name not in authors:\n",
    "                authors.append(name)\n",
    "extracted[\"authors\"] = authors\n",
    "\n",
    "# --- Rating ---\n",
    "rating = None\n",
    "rating_count = None\n",
    "if schema_data and \"aggregateRating\" in schema_data:\n",
    "    agg = schema_data[\"aggregateRating\"]\n",
    "    rating = agg.get(\"ratingValue\")\n",
    "    rating_count = agg.get(\"ratingCount\")\n",
    "extracted[\"rating\"] = rating\n",
    "extracted[\"rating_count\"] = rating_count\n",
    "\n",
    "# --- Description ---\n",
    "description = None\n",
    "for meta_name in [\"description\", \"og:description\", \"twitter:description\"]:\n",
    "    meta = soup.find(\"meta\", attrs={\"name\": meta_name}) or soup.find(\"meta\", property=meta_name)\n",
    "    if meta and meta.get(\"content\"):\n",
    "        description = meta[\"content\"]\n",
    "        break\n",
    "extracted[\"description\"] = description\n",
    "\n",
    "# --- Genres ---\n",
    "genres = []\n",
    "# Try to find genre links or tags\n",
    "for genre_tag in soup.select(\"a[href*='/genres/'], span.BookPageGenreLink, a.BookPageGenreLink\"):\n",
    "    genre = genre_tag.get_text(strip=True)\n",
    "    if genre and genre not in genres:\n",
    "        genres.append(genre)\n",
    "# Sometimes genres are in divs with role=\"list\" and data-testid=\"genresList\"\n",
    "for genre_tag in soup.select(\"[data-testid='genresList'] a, [data-testid='genresList'] span\"):\n",
    "    genre = genre_tag.get_text(strip=True)\n",
    "    if genre and genre not in genres:\n",
    "        genres.append(genre)\n",
    "extracted[\"genres\"] = genres\n",
    "\n",
    "# --- Number of Pages ---\n",
    "num_pages = None\n",
    "if schema_data and \"numberOfPages\" in schema_data:\n",
    "    num_pages = schema_data[\"numberOfPages\"]\n",
    "if not num_pages:\n",
    "    # Try to find in text\n",
    "    m = re.search(r\"(\\d+)\\s+pages\", html)\n",
    "    if m:\n",
    "        num_pages = m.group(1)\n",
    "extracted[\"number_of_pages\"] = num_pages\n",
    "\n",
    "# --- Published Data ---\n",
    "published = None\n",
    "if schema_data and \"datePublished\" in schema_data:\n",
    "    published = schema_data[\"datePublished\"]\n",
    "if not published:\n",
    "    # Try to find in text\n",
    "    pub_match = re.search(r\"Published\\s+([A-Za-z]+\\s+\\d{1,2}[a-z]{0,2},?\\s+\\d{4})\", html)\n",
    "    if pub_match:\n",
    "        published = pub_match.group(1)\n",
    "extracted[\"published\"] = published\n",
    "\n",
    "# --- ISBN ---\n",
    "isbn = None\n",
    "if schema_data and \"isbn\" in schema_data:\n",
    "    isbn = schema_data[\"isbn\"]\n",
    "else:\n",
    "    # Try to find ISBN in text\n",
    "    m = re.search(r\"ISBN(?:13)?:?\\s*([\\d\\-]+)\", html)\n",
    "    if m:\n",
    "        isbn = m.group(1)\n",
    "extracted[\"isbn\"] = isbn\n",
    "\n",
    "# --- Original Title ---\n",
    "original_title = None\n",
    "# Look for \"Original Title\" label\n",
    "orig_title_elem = soup.find(string=re.compile(r\"Original Title\", re.I))\n",
    "if orig_title_elem:\n",
    "    parent = orig_title_elem.find_parent()\n",
    "    if parent:\n",
    "        next_sib = parent.find_next_sibling()\n",
    "        if next_sib:\n",
    "            original_title = next_sib.get_text(strip=True)\n",
    "extracted[\"original_title\"] = original_title\n",
    "\n",
    "# --- Edition Details ---\n",
    "edition_details = None\n",
    "for h4 in soup.find_all(\"h4\", class_=\"Text__title4\"):\n",
    "    if \"This edition\" in h4.get_text():\n",
    "        # Get next sibling or parent block\n",
    "        sib = h4.find_next_sibling()\n",
    "        if sib:\n",
    "            edition_details = sib.get_text(strip=True)\n",
    "        break\n",
    "extracted[\"edition_details\"] = edition_details\n",
    "\n",
    "# --- Other Editions Links ---\n",
    "other_editions_links = []\n",
    "for h4 in soup.find_all(\"h4\", class_=\"Text__title4\"):\n",
    "    if \"More editions\" in h4.get_text():\n",
    "        # Find links under this section\n",
    "        for a in h4.find_all_next(\"a\", href=True, limit=10):\n",
    "            if \"/work/editions/\" in a[\"href\"]:\n",
    "                other_editions_links.append(a[\"href\"])\n",
    "        break\n",
    "extracted[\"other_editions_links\"] = other_editions_links\n",
    "\n",
    "# --- More Information Tags ---\n",
    "more_info = {}\n",
    "for h4 in soup.find_all(\"h4\", class_=\"Text__title4\"):\n",
    "    if \"More information\" in h4.get_text():\n",
    "        # Get next sibling or parent block\n",
    "        sib = h4.find_next_sibling()\n",
    "        if sib:\n",
    "            for li in sib.find_all(\"li\"):\n",
    "                key = li.find(\"span\")\n",
    "                val = li.find(\"div\")\n",
    "                if key and val:\n",
    "                    more_info[key.get_text(strip=True)] = val.get_text(strip=True)\n",
    "        break\n",
    "extracted[\"more_information\"] = more_info\n",
    "\n",
    "# --- Book Statistics Tags ---\n",
    "book_stats = {}\n",
    "# Try to find stats near \"Ratings & Reviews\"\n",
    "for h2 in soup.find_all(\"h2\", class_=\"Text__title2\"):\n",
    "    if \"Ratings & Reviews\" in h2.get_text():\n",
    "        stats_block = h2.find_next(\"div\")\n",
    "        if stats_block:\n",
    "            for stat in stats_block.find_all(\"span\"):\n",
    "                txt = stat.get_text(strip=True)\n",
    "                if re.match(r\"[\\d,]+\", txt):\n",
    "                    book_stats.setdefault(\"stats\", []).append(txt)\n",
    "        break\n",
    "extracted[\"book_statistics\"] = book_stats\n",
    "\n",
    "# --- People read or reading ---\n",
    "people_reading = []\n",
    "for h3 in soup.find_all(\"h3\", class_=\"Text__title3\"):\n",
    "    if \"Friends & Following\" in h3.get_text():\n",
    "        # Find user avatars or names in this section\n",
    "        section = h3.find_parent()\n",
    "        if section:\n",
    "            for user in section.find_all(\"a\", href=True):\n",
    "                if \"/user/show/\" in user[\"href\"]:\n",
    "                    name = user.get_text(strip=True)\n",
    "                    if name:\n",
    "                        people_reading.append(name)\n",
    "        break\n",
    "extracted[\"people_reading\"] = people_reading\n",
    "\n",
    "# --- Suggested Books Comments ---\n",
    "suggested_books = []\n",
    "for h3 in soup.find_all(\"h3\", class_=\"Text__title3\"):\n",
    "    if \"Readers also enjoyed\" in h3.get_text():\n",
    "        section = h3.find_parent()\n",
    "        if section:\n",
    "            for a in section.find_all(\"a\", href=True):\n",
    "                if \"/book/show/\" in a[\"href\"]:\n",
    "                    suggested_books.append({\n",
    "                        \"title\": a.get_text(strip=True),\n",
    "                        \"url\": a[\"href\"]\n",
    "                    })\n",
    "        break\n",
    "extracted[\"suggested_books\"] = suggested_books\n",
    "\n",
    "# --- Comments/Discussion ---\n",
    "comments = []\n",
    "for h2 in soup.find_all(\"h2\", class_=\"Text__h2\"):\n",
    "    if \"Join the discussion\" in h2.get_text():\n",
    "        section = h2.find_parent()\n",
    "        if section:\n",
    "            for comment in section.find_all(\"div\", class_=re.compile(\"Comment\")):\n",
    "                txt = comment.get_text(strip=True)\n",
    "                if txt:\n",
    "                    comments.append(txt)\n",
    "        break\n",
    "extracted[\"comments\"] = comments\n",
    "\n",
    "# --- Print or Save the extracted data ---\n",
    "import pprint\n",
    "pprint.pprint(extracted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ba79c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': None, 'authors': None, 'image_src': None, 'description': None, 'genres': None, 'minirating': None}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Set up Selenium WebDriver (headless for efficiency)\n",
    "chrome_options = Options()\n",
    "chrome_binary = \"/opt/google/chrome/chrome\"\n",
    "chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "chrome_options.add_argument('--headless')         # Run Chrome without GUI\n",
    "chrome_options.add_argument('--disable-gpu') \n",
    "\n",
    "driver = uc.Chrome(options=chrome_options, browser_executable_path=chrome_binary)\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def safe_get_text(element):\n",
    "    return element.text.strip() if element else None\n",
    "\n",
    "try:\n",
    "    url = \"https://www.goodreads.com/book/show/17912916-data-science-for-business?from_search=true&from_srp=true&qid=EVSW0x6JUR&rank=1\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Let the page load\n",
    "\n",
    "    # Click all 'more' and 'Show more' buttons\n",
    "    while True:\n",
    "        buttons = driver.find_elements(By.XPATH, \"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'more')]\")\n",
    "        clickable_buttons = [btn for btn in buttons if btn.is_displayed() and btn.is_enabled()]\n",
    "        if not clickable_buttons:\n",
    "            break\n",
    "        for btn in clickable_buttons:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                time.sleep(0.5)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    info = {}\n",
    "\n",
    "    # Title\n",
    "    try:\n",
    "        title_elem = driver.find_element(By.XPATH, \"//h1[@data-testid='bookTitle']\")\n",
    "        info[\"title\"] = safe_get_text(title_elem)\n",
    "    except Exception:\n",
    "        info[\"title\"] = None\n",
    "\n",
    "    # Authors\n",
    "    authors = []\n",
    "    try:\n",
    "        author_elems = driver.find_elements(By.XPATH, \"//a[contains(@class, 'ContributorLink')]//span[@data-testid='name']\")\n",
    "        for a in author_elems:\n",
    "            name = safe_get_text(a)\n",
    "            if name:\n",
    "                authors.append(name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    info[\"authors\"] = authors if authors else None\n",
    "\n",
    "    # Book image\n",
    "    try:\n",
    "        img_elem = driver.find_element(By.XPATH, \"//img[contains(@class, 'ResponsiveImage')]\")\n",
    "        info[\"image_src\"] = img_elem.get_attribute(\"src\")\n",
    "    except Exception:\n",
    "        info[\"image_src\"] = None\n",
    "\n",
    "    # Description (from meta tag for reliability)\n",
    "    try:\n",
    "        desc_elem = driver.find_element(By.XPATH, \"//meta[@name='description']\")\n",
    "        info[\"description\"] = desc_elem.get_attribute(\"content\")\n",
    "    except Exception:\n",
    "        info[\"description\"] = None\n",
    "\n",
    "    # Genres\n",
    "    genres = []\n",
    "    try:\n",
    "        genre_elems = driver.find_elements(By.XPATH, \"//a[@data-testid='genreLink']\")\n",
    "        for g in genre_elems:\n",
    "            genre = safe_get_text(g)\n",
    "            if genre:\n",
    "                genres.append(genre)\n",
    "    except Exception:\n",
    "        pass\n",
    "    info[\"genres\"] = genres if genres else None\n",
    "\n",
    "    # Ratings (from schema.org script tag)\n",
    "    try:\n",
    "        # Find the <script type=\"application/ld+json\"> with Book schema\n",
    "        script_elems = driver.find_elements(By.XPATH, \"//script[@type='application/ld+json']\")\n",
    "        schema_data = None\n",
    "        for s in script_elems:\n",
    "            try:\n",
    "                data = json.loads(s.get_attribute(\"innerHTML\"))\n",
    "                if isinstance(data, dict) and data.get(\"@type\") == \"Book\":\n",
    "                    schema_data = data\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if schema_data:\n",
    "            agg = schema_data.get(\"aggregateRating\", {})\n",
    "            info[\"ratingValue\"] = agg.get(\"ratingValue\")\n",
    "            info[\"ratingCount\"] = agg.get(\"ratingCount\")\n",
    "            info[\"numberOfPages\"] = schema_data.get(\"numberOfPages\")\n",
    "            info[\"isbn\"] = schema_data.get(\"isbn\")\n",
    "            info[\"schema_org_name\"] = schema_data.get(\"name\")\n",
    "            # Authors from schema\n",
    "            if \"author\" in schema_data:\n",
    "                schema_authors = schema_data[\"author\"]\n",
    "                if isinstance(schema_authors, list):\n",
    "                    info[\"schema_authors\"] = [a.get(\"name\") for a in schema_authors if \"name\" in a]\n",
    "                elif isinstance(schema_authors, dict):\n",
    "                    info[\"schema_authors\"] = [schema_authors.get(\"name\")]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Minirating (try to find a rating summary)\n",
    "    try:\n",
    "        minirating_elem = driver.find_element(By.XPATH, \"//span[contains(@class, 'Text__body3')]\")\n",
    "        info[\"minirating\"] = safe_get_text(minirating_elem)\n",
    "    except Exception:\n",
    "        info[\"minirating\"] = None\n",
    "\n",
    "    # Details (pages, publication, etc.)\n",
    "    details = {}\n",
    "    try:\n",
    "        details_section = driver.find_element(By.XPATH, \"//div[@data-testid='bookDetails']\")\n",
    "        rows = details_section.find_elements(By.XPATH, \"./div\")\n",
    "        for row in rows:\n",
    "            try:\n",
    "                label = row.find_element(By.XPATH, \".//div[contains(@class, 'BookDetailsRow__label')]\")\n",
    "                value = row.find_element(By.XPATH, \".//div[contains(@class, 'BookDetailsRow__value')]\")\n",
    "                details[safe_get_text(label)] = safe_get_text(value)\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception:\n",
    "        pass\n",
    "    if details:\n",
    "        info[\"details\"] = details\n",
    "\n",
    "    # Original title, edition details, more editions, more info, book stats, etc.\n",
    "    # These are often in <h4> or <div> blocks with nearby text\n",
    "    try:\n",
    "        h4s = driver.find_elements(By.XPATH, \"//h4[contains(@class, 'Text__title4')]\")\n",
    "        for h4 in h4s:\n",
    "            h4_text = safe_get_text(h4)\n",
    "            if not h4_text:\n",
    "                continue\n",
    "            if \"This edition\" in h4_text:\n",
    "                # Edition details are likely in the next sibling\n",
    "                try:\n",
    "                    edition_details = h4.find_element(By.XPATH, \"following-sibling::*[1]\")\n",
    "                    info[\"edition_details\"] = safe_get_text(edition_details)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif \"More editions\" in h4_text:\n",
    "                try:\n",
    "                    more_editions = h4.find_element(By.XPATH, \"following-sibling::*[1]\")\n",
    "                    links = more_editions.find_elements(By.XPATH, \".//a\")\n",
    "                    info[\"more_editions_links\"] = [l.get_attribute(\"href\") for l in links if l.get_attribute(\"href\")]\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif \"More information\" in h4_text:\n",
    "                try:\n",
    "                    more_info = h4.find_element(By.XPATH, \"following-sibling::*[1]\")\n",
    "                    info[\"more_information\"] = safe_get_text(more_info)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Book statistics (ratings, pages, etc. already included above)\n",
    "\n",
    "    # People read or reading (look for \"Friends & Following\" or similar)\n",
    "    try:\n",
    "        friends_elem = driver.find_element(By.XPATH, \"//h3[contains(@class, 'Text__title3') and contains(., 'Friends & Following')]\")\n",
    "        info[\"friends_and_following_section\"] = safe_get_text(friends_elem)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Suggested books comments (\"Readers also enjoyed\")\n",
    "    try:\n",
    "        readers_enjoyed_elem = driver.find_element(By.XPATH, \"//h3[contains(@class, 'Text__title3') and contains(., 'Readers also enjoyed')]\")\n",
    "        # Get book links in this section\n",
    "        parent = readers_enjoyed_elem.find_element(By.XPATH, \"following-sibling::*[1]\")\n",
    "        links = parent.find_elements(By.XPATH, \".//a\")\n",
    "        info[\"readers_also_enjoyed_links\"] = [l.get_attribute(\"href\") for l in links if l.get_attribute(\"href\")]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Comments/discussion (\"Join the discussion\")\n",
    "    try:\n",
    "        discussion_elem = driver.find_element(By.XPATH, \"//h2[contains(@class, 'Text__h2') and contains(., 'Join the discussion')]\")\n",
    "        info[\"discussion_section\"] = safe_get_text(discussion_elem)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(info)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca25d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": null,\n",
      "  \"authors\": [],\n",
      "  \"description\": null,\n",
      "  \"genres\": [],\n",
      "  \"edition_details\": null,\n",
      "  \"more_editions_links\": [],\n",
      "  \"more_information\": null,\n",
      "  \"friends_and_following_section\": null,\n",
      "  \"readers_also_enjoyed_links\": [],\n",
      "  \"discussion_section\": null,\n",
      "  \"rating_value\": null,\n",
      "  \"rating_count\": null,\n",
      "  \"number_of_pages\": null,\n",
      "  \"isbn\": null,\n",
      "  \"published_date\": null,\n",
      "  \"original_title\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Playwright async code to extract book info from Goodreads\n",
    "\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import json\n",
    "import re\n",
    "\n",
    "url = \"https://www.goodreads.com/book/show/17912916-data-science-for-business?from_search=true&from_srp=true&qid=EVSW0x6JUR&rank=1\"\n",
    "\n",
    "async def extract_schema_org_data(page):\n",
    "    # Extract the Schema.org JSON-LD script\n",
    "    scripts = await page.query_selector_all('script[type=\"application/ld+json\"]')\n",
    "    for script in scripts:\n",
    "        try:\n",
    "            data = json.loads(await script.inner_text())\n",
    "            if isinstance(data, dict) and data.get(\"@type\") == \"Book\":\n",
    "                return data\n",
    "        except Exception:\n",
    "            continue\n",
    "    return {}\n",
    "\n",
    "async def safe_text(el):\n",
    "    try:\n",
    "        return (await el.inner_text()).strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "async def safe_attr(el, attr):\n",
    "    try:\n",
    "        return await el.get_attribute(attr)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "async def main():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # --- TITLE ---\n",
    "        # Try h1 first\n",
    "        title_el = await page.query_selector('h1.Text.Text__title1[data-testid=\"bookTitle\"]')\n",
    "        info[\"title\"] = await safe_text(title_el) if title_el else None\n",
    "\n",
    "        # Try meta tags as backup\n",
    "        if not info[\"title\"]:\n",
    "            meta_title = await page.query_selector('meta[property=\"og:title\"]')\n",
    "            if not meta_title:\n",
    "                meta_title = await page.query_selector('meta[name=\"twitter:title\"]')\n",
    "            info[\"title\"] = await safe_attr(meta_title, \"content\") if meta_title else None\n",
    "\n",
    "        # Try Schema.org as last resort\n",
    "        schema = await extract_schema_org_data(page)\n",
    "        if not info[\"title\"] and schema:\n",
    "            info[\"title\"] = schema.get(\"name\")\n",
    "\n",
    "        # --- AUTHORS ---\n",
    "        authors = []\n",
    "        # Try to get authors from the visible page\n",
    "        # The author block is under h3.Text.Text__title3.Text__regular, then a.ContributorLink > span.ContributorLink__name[data-testid=\"name\"]\n",
    "        author_block = await page.query_selector('h3.Text.Text__title3.Text__regular:has-text(\"Author\")')\n",
    "        if author_block:\n",
    "            # Find all following sibling a.ContributorLink > span.ContributorLink__name[data-testid=\"name\"]\n",
    "            author_spans = await page.query_selector_all('a.ContributorLink span.ContributorLink__name[data-testid=\"name\"]')\n",
    "            for a in author_spans:\n",
    "                name = await safe_text(a)\n",
    "                if name and name not in authors:\n",
    "                    authors.append(name)\n",
    "        else:\n",
    "            # Fallback: try all a.ContributorLink span.ContributorLink__name[data-testid=\"name\"]\n",
    "            author_spans = await page.query_selector_all('a.ContributorLink span.ContributorLink__name[data-testid=\"name\"]')\n",
    "            for a in author_spans:\n",
    "                name = await safe_text(a)\n",
    "                if name and name not in authors:\n",
    "                    authors.append(name)\n",
    "        # Fallback: try Schema.org\n",
    "        if not authors and schema:\n",
    "            schema_authors = schema.get(\"author\", [])\n",
    "            if isinstance(schema_authors, dict):\n",
    "                schema_authors = [schema_authors]\n",
    "            for a in schema_authors:\n",
    "                if isinstance(a, dict) and a.get(\"@type\") == \"Person\" and a.get(\"name\"):\n",
    "                    authors.append(a[\"name\"])\n",
    "        info[\"authors\"] = authors\n",
    "\n",
    "        # --- DESCRIPTION ---\n",
    "        desc = None\n",
    "        for sel in [\n",
    "            'meta[name=\"description\"]',\n",
    "            'meta[property=\"og:description\"]',\n",
    "            'meta[name=\"twitter:description\"]'\n",
    "        ]:\n",
    "            desc_meta = await page.query_selector(sel)\n",
    "            if desc_meta:\n",
    "                desc = await safe_attr(desc_meta, \"content\")\n",
    "                if desc:\n",
    "                    break\n",
    "        info[\"description\"] = desc\n",
    "\n",
    "        # --- GENRES ---\n",
    "        # Try to find genre chips or similar\n",
    "        genres = []\n",
    "        # Try to find all links to /genres/ that are visible\n",
    "        genre_els = await page.query_selector_all('a[href*=\"/genres/\"]')\n",
    "        for g in genre_els:\n",
    "            genre = await safe_text(g)\n",
    "            if genre and genre not in genres:\n",
    "                genres.append(genre)\n",
    "        # If not found, try to find genre chips by class (sometimes genres are in span or div with role=\"listitem\")\n",
    "        if not genres:\n",
    "            genre_chips = await page.query_selector_all('[data-testid=\"genresList\"] [role=\"listitem\"]')\n",
    "            for chip in genre_chips:\n",
    "                genre = await safe_text(chip)\n",
    "                if genre and genre not in genres:\n",
    "                    genres.append(genre)\n",
    "        info[\"genres\"] = genres\n",
    "\n",
    "        # --- EDITION DETAILS ---\n",
    "        # Look for h4.Text.Text__title4:has-text(\"This edition\")\n",
    "        edition_details = None\n",
    "        h4_edition = await page.query_selector('h4.Text.Text__title4:has-text(\"This edition\")')\n",
    "        if h4_edition:\n",
    "            sibling = await h4_edition.evaluate_handle(\"el => el.nextElementSibling\")\n",
    "            if sibling:\n",
    "                edition_details = await page.evaluate(\"el => el && el.innerText\", sibling)\n",
    "        info[\"edition_details\"] = edition_details\n",
    "\n",
    "        # --- MORE EDITIONS LINKS ---\n",
    "        more_editions_links = []\n",
    "        h4_more_editions = await page.query_selector('h4.Text.Text__title4:has-text(\"More editions\")')\n",
    "        if h4_more_editions:\n",
    "            sibling = await h4_more_editions.evaluate_handle(\"el => el.nextElementSibling\")\n",
    "            if sibling:\n",
    "                # Only get <a> tags with hrefs that look like book links\n",
    "                links = await page.query_selector_all('h4.Text.Text__title4:has-text(\"More editions\") + * a[href]')\n",
    "                for l in links:\n",
    "                    href = await safe_attr(l, \"href\")\n",
    "                    if href and \"/book/show/\" in href and href not in more_editions_links:\n",
    "                        more_editions_links.append(href)\n",
    "        info[\"more_editions_links\"] = more_editions_links\n",
    "\n",
    "        # --- MORE INFORMATION ---\n",
    "        more_information = None\n",
    "        h4_more_info = await page.query_selector('h4.Text.Text__title4:has-text(\"More information\")')\n",
    "        if h4_more_info:\n",
    "            sibling = await h4_more_info.evaluate_handle(\"el => el.nextElementSibling\")\n",
    "            if sibling:\n",
    "                more_information = await page.evaluate(\"el => el && el.innerText\", sibling)\n",
    "        info[\"more_information\"] = more_information\n",
    "\n",
    "        # --- FRIENDS & FOLLOWING ---\n",
    "        friends_section = None\n",
    "        friends_h3 = await page.query_selector('h3.Text.Text__title3:has-text(\"Friends & Following\")')\n",
    "        if friends_h3:\n",
    "            # Try to get the next sibling, which may contain the list or count\n",
    "            sibling = await friends_h3.evaluate_handle(\"el => el.nextElementSibling\")\n",
    "            if sibling:\n",
    "                friends_section = await page.evaluate(\"el => el && el.innerText\", sibling)\n",
    "            else:\n",
    "                friends_section = await safe_text(friends_h3)\n",
    "        info[\"friends_and_following_section\"] = friends_section\n",
    "\n",
    "        # --- READERS ALSO ENJOYED ---\n",
    "        readers_links = []\n",
    "        readers_h3 = await page.query_selector('h3.Text.Text__title3:has-text(\"Readers also enjoyed\")')\n",
    "        if readers_h3:\n",
    "            sibling = await readers_h3.evaluate_handle(\"el => el.nextElementSibling\")\n",
    "            if sibling:\n",
    "                # Only get <a> tags with hrefs that look like book links\n",
    "                links = await page.query_selector_all('h3.Text.Text__title3:has-text(\"Readers also enjoyed\") + * a[href]')\n",
    "                for l in links:\n",
    "                    href = await safe_attr(l, \"href\")\n",
    "                    if href and \"/book/show/\" in href and href not in readers_links:\n",
    "                        readers_links.append(href)\n",
    "        info[\"readers_also_enjoyed_links\"] = readers_links\n",
    "\n",
    "        # --- DISCUSSION SECTION ---\n",
    "        discussion_section = None\n",
    "        discussion_h2 = await page.query_selector('h2.Text.Text__h2:has-text(\"Join the discussion\")')\n",
    "        if discussion_h2:\n",
    "            sibling = await discussion_h2.evaluate_handle(\"el => el.nextElementSibling\")\n",
    "            if sibling:\n",
    "                discussion_section = await page.evaluate(\"el => el && el.innerText\", sibling)\n",
    "            else:\n",
    "                discussion_section = await safe_text(discussion_h2)\n",
    "        info[\"discussion_section\"] = discussion_section\n",
    "\n",
    "        # --- BOOK STATISTICS (ratings, pages, etc.) ---\n",
    "        # Use schema.org if available\n",
    "        if schema:\n",
    "            info[\"rating_value\"] = schema.get(\"aggregateRating\", {}).get(\"ratingValue\")\n",
    "            info[\"rating_count\"] = schema.get(\"aggregateRating\", {}).get(\"ratingCount\")\n",
    "            info[\"number_of_pages\"] = schema.get(\"numberOfPages\")\n",
    "            info[\"isbn\"] = schema.get(\"isbn\")\n",
    "            info[\"published_date\"] = schema.get(\"datePublished\")\n",
    "            info[\"original_title\"] = schema.get(\"name\")\n",
    "        else:\n",
    "            info[\"rating_value\"] = None\n",
    "            info[\"rating_count\"] = None\n",
    "            info[\"number_of_pages\"] = None\n",
    "            info[\"isbn\"] = None\n",
    "            info[\"published_date\"] = None\n",
    "            info[\"original_title\"] = None\n",
    "\n",
    "        # Try to get rating from visible page if not in schema\n",
    "        if not info.get(\"rating_value\"):\n",
    "            # Try to find the rating value near the \"Ratings & Reviews\" h2\n",
    "            rating_value = None\n",
    "            rating_h2 = await page.query_selector('h2.Text.Text__title2:has-text(\"Ratings & Reviews\")')\n",
    "            if rating_h2:\n",
    "                # Look for a span with data-testid=\"ratingStar\" or similar nearby\n",
    "                rating_span = await page.query_selector('span[data-testid=\"ratingStar\"]')\n",
    "                if rating_span:\n",
    "                    rating_value = await safe_text(rating_span)\n",
    "                else:\n",
    "                    # Try to find a span with class \"RatingStatistics__rating\"\n",
    "                    rating_span = await page.query_selector('span.RatingStatistics__rating')\n",
    "                    if rating_span:\n",
    "                        rating_value = await safe_text(rating_span)\n",
    "            info[\"rating_value\"] = rating_value\n",
    "\n",
    "        # Print the extracted info\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "# If running in a notebook, use asyncio.run only if not already in an event loop\n",
    "import sys\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    asyncio.run(main())\n",
    "else:\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            task = loop.create_task(main())\n",
    "        else:\n",
    "            loop.run_until_complete(main())\n",
    "    except RuntimeError:\n",
    "        asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c1bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
